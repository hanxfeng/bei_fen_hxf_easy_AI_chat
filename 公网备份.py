import os
from flask import Flask, request, render_template, jsonify, Response
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, \
    StoppingCriteriaList, TextIteratorStreamer
from sentence_transformers import SentenceTransformer
import torch
import faiss
import json
from datetime import datetime
import socketio
import requests
import threading

#æ•°æ®åº“ä½ç½®
data_path = "templates/train.json"

#æ¨¡å‹ä½ç½®
model_path = 'models/Qwen3-17B'

#äººè®¾ä½ç½®
re_she_path = "templates/ren_she.txt"

#RAGæ¨¡å‹ä½ç½®
rag_model_path = "models/m3e-base"

#RAGç´¢å¼•ä½ç½®
index_path = "templates/index.faiss"

#èŠå¤©è®°å½•æ•°æ®ä½ç½®
ji_lu_path = "templates/ji_lu.json"

#å…¬ç½‘æœåŠ¡å™¨ipåœ°å€
serve_path = 'http://101.200.161.243:8080'

SECRET_TOKEN = "your-super-secret-token"

#åŠ è½½æ•°æ®åº“
with open(data_path, "r", encoding="utf-8") as f:
    knowledge_data = json.load(f)

#æ•´ç†æ•°æ®
documents = [
    f"é—®é¢˜ï¼š{item.get('instruction', '')}\nå›ç­”ï¼š{item.get('output', '')}"
    for item in knowledge_data
]

#åŠ è½½äººè®¾æ•°æ®
with open(re_she_path, "r", encoding="utf-8") as f:
    re_she = f.read()

# å¯¼å…¥FAISSç´¢å¼•
index = faiss.read_index(index_path)

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

model_name = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    device_map="auto",
).to("cuda")

tokenizer = AutoTokenizer.from_pretrained(model_path)

embedding_model = SentenceTransformer(rag_model_path)


def chat_completions_model(messages, max_tokens=500, temperature=0.1):
    #åŠ è½½å†å²èŠå¤©è®°å½•
    with open(ji_lu_path, 'r', encoding='utf-8') as file:
        ji_lu = json.load(file)
    #è°ƒæ•´æ•°æ®æ ¼å¼
    documents_ji_lu = [
        f"é—®é¢˜ï¼š{item.get('instruction', '')}\næ—¶é—´ï¼š{item.get('time', '')}\nå›ç­”ï¼š{item.get('output', '')}"
        for item in ji_lu
    ]

    q = messages

    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]

    #åœ¨æ•°æ®åº“ä¸­æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„æ•°æ®
    user_question = messages[-1]["content"]
    question_embedding = embedding_model.encode([user_question])
    D, I = index.search(question_embedding, k=3)
    related_docs = "\n---\n".join([documents[i] for i in I[0]])

    #åœ¨å†å²èŠå¤©è®°å½•ä¸­æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„æ•°æ®
    question_embedding_jl = embedding_model.encode([user_question])
    D, I = index.search(question_embedding_jl, k=3)
    related_docs_jl = "\n---\n".join([documents_ji_lu[i] for i in I[0]])

    #æ„å»ºæç¤ºè¯
    rag_prefix = (
        "è¯·ä½ æ ¹æ®ä»¥ä¸‹æä¾›çš„è®°å½•ä¸ç”¨æˆ·äº¤æµã€‚\n"
        "è®°å½•å¦‚ä¸‹ï¼Œå¦‚æœä¸ç”¨æˆ·è¾“å…¥ä¸ç›¸å…³åˆ™ä¸éœ€è¦è¿›è¡Œå‚è€ƒï¼š\n"
        f"{related_docs}\n"
        "å›ç­”æ—¶ä¹Ÿå¯ä»¥å‚è€ƒä»¥ä¸‹å†å²èŠå¤©è®°å½•ï¼Œå¦‚ä¸ç›¸å…³ä¹Ÿå¯ä¸å‚è€ƒ\n"
        "èŠå¤©è®°å½•å¦‚ä¸‹ï¼Œå…¶ä¸­instructionæ˜¯ç”¨æˆ·çš„è¾“å…¥ï¼Œtimeæ˜¯ç”¨æˆ·è¾“å…¥æ—¶çš„æ—¶é—´ï¼Œoutputæ˜¯æ¨¡å‹æ ¹æ®ç”¨æˆ·è¾“å…¥è€Œè¾“å‡ºçš„å†…å®¹"
        f"{related_docs_jl}\n"
        "å›ç­”æ—¶è¯·å°½é‡åŸºäºä¸Šè¿°å†…å®¹ï¼Œå¹¶é¿å…ç¼–é€ ã€‚åŒæ—¶åœ¨å›ç­”æ—¶è¦ä¸ä»¥ä¸‹äººè®¾ç›¸ç¬¦\n"
        "äººè®¾å¦‚ä¸‹\n"
        f"{re_she}"
    )
    system_prompt = {
        "role": "system",
        "content": rag_prefix
    }

    full_conversation = [system_prompt] + messages

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    #æ¨¡æ¿åŒ–æ•°æ®
    formatted_input = tokenizer.apply_chat_template(
        full_conversation,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )
    inputs = tokenizer(formatted_input, return_tensors='pt').to('cuda')

    input_tokens = inputs.input_ids.shape[1]

    #è¿›è¡Œæ¨ç†
    outputs = model_name.generate(
        inputs.input_ids,
        max_length=input_tokens + max_tokens,
        temperature=temperature,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if "</think>" in response:
        response = response.split("</think>")[-1].strip()
    else:
        response = response.strip()

    return response, q


sio = socketio.Client(logger=True, engineio_logger=True)


@sio.event
def connect():
    print("âœ… æˆåŠŸè¿æ¥åˆ°è½¬å‘æœåŠ¡å™¨")


@sio.event
def disconnect():
    print("âš ï¸ ä¸è½¬å‘æœåŠ¡å™¨æ–­å¼€è¿æ¥")


# === æ¥æ”¶æ¨ç†è¯·æ±‚ ===
@sio.on("infer_request")
def handle_infer_request(data):
    print("ğŸ“¥ æ”¶åˆ°æ¨ç†è¯·æ±‚:", data)
    text = data.get("text", "")
    task_id = data.get("task_id")

    # å¼€æ–°çº¿ç¨‹æ‰§è¡Œæ¨ç†ï¼Œé˜²æ­¢é˜»å¡ Socket.IO å¿ƒè·³
    thread = threading.Thread(
        target=process_inference,
        args=(task_id, text),
        daemon=True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
    )
    thread.start()

def save_record(new_entry):
    """å°†æ–°è®°å½•ä¿å­˜åˆ° ji_lu.jsonï¼Œä¿æŒä¸ºæ ‡å‡† JSON æ•°ç»„"""
    path = "templates/ji_lu.json"
    data = []

    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = []  # æ–‡ä»¶åæ‰å°±é‡ç½®ä¸ºç©ºæ•°ç»„

    data.append(new_entry)

    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def process_inference(task_id, text):
    """åå°æ‰§è¡Œæ¨ç†ä»»åŠ¡å¹¶å›ä¼ ç»“æœ"""
    try:
        # è°ƒç”¨ä½ çš„æ¨ç†å‡½æ•°
        ai_response, _ = chat_completions_model(messages=text, temperature=0.9)

        # æ–°æ—¥å¿—æ¡ç›®
        new_entry = {
            "instruction": text,
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "output": ai_response
        }

        # âœ… ç”¨æ ‡å‡† JSON æ•°ç»„æ–¹å¼ä¿å­˜
        save_record(new_entry)

        # å‘é€æ¨ç†ç»“æœå›è½¬å‘æœåŠ¡å™¨
        sio.emit("infer_response", {"task_id": task_id, "response": ai_response})
        print(f"ä»»åŠ¡ {task_id} æ¨ç†å®Œæˆï¼Œå·²å‘é€ç»“æœ")

    except Exception as e:
        print(f"ä»»åŠ¡ {task_id} æ¨ç†å¤±è´¥: {e}")
        sio.emit("infer_response", {"task_id": task_id, "response": f"æ¨ç†å¤±è´¥: {str(e)}"})


# === ä¸è½¬å‘æœåŠ¡å™¨å»ºç«‹è¿æ¥ ===
sio.connect(
    serve_path,
    auth={"token": SECRET_TOKEN},   # æºå¸¦ token è®¤è¯
    wait_timeout=30,
    transports=["websocket", "polling"]  # ä¼˜å…ˆç”¨ websocketï¼Œå¤±è´¥å†å›é€€åˆ° polling
)

sio.wait()

